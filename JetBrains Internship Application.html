Task #<!-- -->1</h3><div class="rs-text-2 rs-text-2_theme_light rs-text-2_paragraph-offset-auto ia-rendered-content HtmlText_content__AoGe8 TaskSolution_taskDefinition__4QUw5"><h1>Implement a retrieval pipeline using an open-source embedding model and evaluate its retrieval quality</h1>

<p><strong>Steps to follow</strong>:</p>

<ol class="rs-ol">
<li><strong>Prepare a dataset</strong>:<br>
Take one of the following datasets from this <a href="https://github.com/brandonstarxel/chunking_evaluation/tree/main/chunking_evaluation/evaluation_framework/general_evaluation_data" class="ia-link">repository</a>: "<em>Chatlogs</em>" / "<em>State of the Union</em>" / "<em>Wikitext</em>".<br>
The dataset should contain:<br>
<ul class="rs-ul"><br>
<li>A corpora (a collection of documents)</li><br>
<li>A set of queries relevant to those documents</li><br>
<li>Relevant excerpts from the corpus that correspond to the queries<br><br>
The text corpora are located in the "<em>corpora</em>" folder and the labeled set of questions with golden excerpts is located in "<em>questions_df.csv</em>". If needed, shrink it further by reducing the corpora and removing the questions with excerpts that correspond to the removed corpora part.</li><br>
</ul></li>
<li><strong>Implement a chunking algorithm</strong>:<br>
Use the "<em>FixedTokenChunker</em>" chunking algorithm. Take the implementation from <a href="https://github.com/brandonstarxel/chunking_evaluation/blob/main/chunking_evaluation/chunking/fixed_token_chunker.py" class="ia-link">here</a>.<br>
Read the detailed explanation of this algorithm in <a href="https://research.trychroma.com/evaluating-chunking#recursivecharactertextsplitter-&amp;-tokentextsplitter-[-]" class="ia-link">the paper</a> and understand how it works.</li>
<li><strong>Define retrieval quality metrics</strong>:<br>
Implement "<em>Precision</em>" and "<em>Recall</em>" metrics for evaluating the retrieval quality based on the definition from <a href="https://research.trychroma.com/evaluating-chunking#metrics" class="ia-link">the paper</a>.<br>
Use <a href="https://github.com/brandonstarxel/chunking_evaluation/blob/main/chunking_evaluation/evaluation_framework/base_evaluation.py" class="ia-link">the paper implementation of them</a> as a reference, if needed.</li>
<li><strong>Prepare an embedding function</strong>:<br>
Select one of the following open-source embedding models: "<em>all-MiniLM-L6-v2</em>" (<a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" class="ia-link">HF model card</a>) or "<em>multi-qa-mpnet-base-dot-v1</em>" (<a href="https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1" class="ia-link">HF model card</a>).<br>
Implement the embedding function with the chosen model that takes a batch of texts and outputs a batch of their corresponding embeddings.</li>
<li><strong>Develop the retrieval evaluation pipeline</strong>:<br>
Write a script that takes the following parameters:<br>
<ul class="rs-ul"><br>
<li>Chunker</li><br>
<li>Embedding function</li><br>
<li>Number of retrieved chunks<br><br>
The script should process the text corpus into chunks, generate embeddings, and compute the chosen retrieval quality metrics on your evaluation dataset with questions and golden excerpts: "Input data → Chunking corpora → Embedding corpora chunks → Embedding query → Retrieval → Evaluation".</li><br>
</ul></li>
<li><strong>Run the evaluation</strong>:<br>
Run your evaluation pipeline.</li>
<li><strong>Experiment with retrieval hyperparameters</strong>:<br>
<ul class="rs-ul"><br>
<li>Experiment with chunking parameters (e.g., a chunk size of 200 and 400 tokens).</li><br>
<li>Experiment with different values for the number of retrieved chunks (e.g., 5 and 10).</li><br>
</ul></li>
<li><strong>Analyze and compare results</strong>:<br>
<ul class="rs-ul"><br>
<li>Summarize the experiments in a comparison table.</li><br>
<li>Review the results and elaborate on key findings.</li><br>
</ul></li>
</ol>

<p><strong>Final submission requirements</strong>:<br>
Your final submission should include:</p>

<ul class="rs-ul">
<li>The dataset with computed metrics of your experiments</li>
<li>All necessary code</li>
<li>A report (in any format) explaining your thought process, findings, and insights</>